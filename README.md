# suzano-test
repository for suzano data engineer test

Para iniciar o desafio, vou quebrar a arquitetura em etapas para poder melhor explicar os desafios e ferramentas e cada etapa...

Para extração temos algumas opções:

A melhor delas e mais automática seria extrair os dados diretamente de APIs, contudo o provedor do website não possui APIs liberadas para uso, mesmo investigando pelas ferramentas do desenvolvedor (do navegador) e encontrando o endereço das APIs, a requisição precisa ser autenticada com isso impossibilita de que possamos extrair diretamente da fonte ou da origem que o site usa para estruturar seus dados e sua página.

A segunda opção que temos seria buscar alguma ferramenta externa ou de terceiros, uma biblioteca ou algo do tipo que possa ler as APIs do cliente, ou seja, a API que ele usa no website já fazendo uma requisição autenticada ou alguma biblioteca feita para extrair dados diretamente do website ou dados similares dos mesmos índices econômicos por exemplo. 

Contudo quando limitamos a consulta dos dados a esse site (investing.com) e percebemos que não podemos fazer nenhum dos desses dois tipos de extração de dados que seria mais automática, temos que chegar numa terceira opção mais complexa.

A terceira opção seria utilizar de bibliotecas do Python que lêem dinamicamente o conteúdo do site e nesse caso simulam cliques e navegações na página pra poder navegar pelo conteúdo e capturar os dados das tabelas e dos gráficos presente dentro do site.

Observação: encontrei ao pesquisar uma biblioteca python chamada Investpy e outra chamada Investiny ambas libs são do mesmo desenvolvedor e servem especificamente pra extrair dados diretamente das APIs origem do site investing.com.
Contudo esse serviço parou de funcionar em meados de 2022 pois o site investing.com em um ponto com mudou suas políticas de segurança, criptografia e autenticação das APIs e não permitiu mais acessos públicos com isso ficamos impossibilitados de extrair os dados de forma automática do site

A última maneira mais perto de automática seria utilizando bibliotecas do como Selenium, que criaria a possibilidade de extrair esses dados como se fosse um usuário indo lá e dando control C control V. Isso funciona bem pra página do indíce da moeda chinesa, mais para as outras páginas não funcionaria muito bem pois lá a gente tem calendários e alguns elementos HTML um pouco mais complexos de serem simulados os cliques movimentações e afins. E pra que pra que a gente possa abrir aquele range de datas do calendário e conseguir extrair o período completo esses movimentos que teriamos que simular ele seriam muito complexos pra fazer essa extração. 
Então quando a gente for consultar o site a gente enfrentariamos alguns problemas pra carregar. Um problema por exemplo seriam os os anúncios, o site possui um pop up de um anúncio inscreva-se em nosso portal e esse anúncio ele vai servir geralmente na maioria das vezes pra que usuários se inscrevam no site então não tem como tirar isso e esse anúncio ele bloqueia a execução da rotina do Selenium porque o mesmo abre a página da internet com driver do Google Chrome e imita um usuário mexendo na página só que a gente não consegue prever quando o anúncio vai aparecer, então esse pop up acabaria atrapalhando em diversos momentos.

Fazendo um breve resumo o código pra poder extrair os dados da página chinesa tem esse problema do pop up mas não é sempre que acontece, contudo para as outras páginas teria que navegar dentro do Calendário que já é bem mais complexo, o código começa a ficar num nível de complexidade muito grande então essa automação entre aspas começa a se tornar problemática porque eu posso ter um dia que aparece um pop up e trava, eu posso ter um dia que dá um problema ali ele não consegue ler e mesmo mesmo pra navegar nesse calendário é complexo teria que fazer muitas etapas, muitas processos, muitos testes. 
Então é uma implementação de muitas horas e que acaba sendo um hard code, pouco automático que poderia ocasionar erros e necessitaria de muitos testes e possíveis execuções manuais. 

Então iremos partir pra uma solução mais elegante que vai ser utilizar de uma arquitetura completamente automática e orquestrada, contudo vamos utilizar um ponto manual, que seria o download dos dados.
Então vamos pegar os dados por mês considerando o inicio do período passado até o dia de hoje. 
Vamos considerar que a gente precisa desses dados até o dia de hoje a gente teria que baixar os dados manualmente todos os dias, o que não é o ideal, mas serviria ao propósito.
Contudo ao baixar os dados manualmente, arquiteturariamos um pipeline que que pudesse fazer essa ingestão de forma automático.

Infelizmente não parece possivel para mim encontrar uma alternativa pra processar mais automático esses dados de forma que a gente consiga consiga automatizar completamente o processo porque o cliente não forneceu APIs e pelo cliente não fornecer APIs impossbilita a extração dos dados de forma automática. 

Para resumir, a nossa solução mais elegante seria automatizar todo processo menos a parte de extração dos dados, não conseguimos por hora automatizar mais do que isso.
Se estivessemos em um cenário da vida real, ambiente produtivo etc, teriamos duas opções para trabalhar:

1- Seria buscar alguma outra fonte dos mesmos dados, dos mesmos índices que liberasse uma API ou acesso à alguma base, alguma coisa do tipo para que pudessemos extrair os índices em tempo real.
2- Um site que já mostrasse todos os índices, todos dados histórico, em que usuário não tivesse que ficar interagindo para buscar os dados específicos  
3- A outra opção seria pedir a esse site (investing.com) que liberasse um tokem e o acesso a APIs para que os dados pudessem ser extraidos, pois se precisamos desses dados possívelmente estamos falando de uma corretora de investimentos ou um banco, alguém que precisa desses dados de índice pra tomar decisões na bolsa de valores ou alguma coisa do tipo.

Então provavelmente teriamos que fazer um pedido de uma integração de sistema ou API que já faz esse acesso automático, teria que existir uma colaboração entre os times dos desenvolvedores do site e o nosso sistema. Esse seria o cenário da vida real que não seria extrair os dados na mão, teriamos que tentar automatizar pra poder conectar nas APIs do cliente. 
Passando desse ponto depois que a gente já passa das APIs do cliente que a gente já teria encerrado essa primeira fase de extração do nosso lado dessa arquitetura.

Não foi citado aqui no desafio por exemplo a volumetria, latência de dados, mas vamos supor que o índice precisasse atualizar por segundo, por exemplo eu precisasse ver o índice da moeda por segundo eu acredito que não seja o caso, mas se eu precisasse ver por segundo o índice da moeda eu já teria um problema, pois eu teria que ter um usuário entrando e baixando a a pasta planilha (o CSV) ali a cada segundo e colocando no sistema pra rodar cada segundo, com isso já teriamos um problema, então o caminho certo seria a automação no caso do nosso exemplo que eu acredito não ter como automatizar mais devido aos problemas citados. 

Eu deixei um exemplo de um código com o Selenio, da moeda chinesa que vai ser utilizado só pra vocês entenderem mais ou menos como seria um código desse, então eu estruturei o código para que ele consiga extrair os dados chineses, porque o tipo de página permite, contudo a página dos outros dois índices já é bem mais complexa e precisa navegar dentro de um calendário como comentado... Com isso acabamos ficando bloqueados ali de fazer essas extraçoes.

Agora seguimos para a parte de transformação e carga dos dados. Ou seja, de qualquer forma precisa inserir-los numa tabela então como vai existir um tratamento de dado a gente já vai estar falando assim uma transformação então a gente vai fazer um pipeline de ET onde a gente extrai os dados ainda assim de forma manual mas automatiza a transformação e o carregamento dos dados tá e aí o cenário de carregamento a gente primeiro cenário de transformação no caso a gente utilizariam data Frame aí eu vou deixar eu vou sempre seguir com três opções aqui você com opção streaming uma opção New Time e uma opção em Beth por exemplo né todos no GC todas as arquiteturas do GCP e eu vou explicando elas é que ao longo do processo né então eu vou sempre mostrar três arquiteturas pra três cenários diferentes de latência de dado e volumetria diferente e no final do desafio claro uma dessas eu vou fazer pra vocês conseguirem ver pra ver funcionava rodar enfim tá então pra transformação a gente começa vamos começar a desenhar o nosso nos nossos três cenários até esse momento né que a gente extraiu e carregou os dados
Agora na parte da do carregamento dos dados tá eu vou apresentar três possíveis arquiteturas pra todo processo tá desde a parte da extração ali então enfim eu vou eu vou começar ali da parte da carga então download do Transformers download Load-vou mostrar três tipos de arquitetura diferente para três cenários diferentes que a gente vai tratar de 
Carregamento vale lembrar né que a extração a gente vai deixar digamos assim entre aspas em branco porque não deu pra automatizar totalmente tá então assim a a a extração vai ser o mesmo pros três cenários eu vou colocar aí que o usuário vai baixar manualmente os arquivos e vai colocar numa pasta que vai ser jogada pra dentro do Call store da GCP ou no caso do cenário perfeito produtiva a gente teria que pedir pro cliente as apps dele né a gente teria que pedir o acesso pedir a liberação das app do cliente ou pedir pra criar 11 serviço integrador dos dados ou procurar alguma outra fonte de dados enfim como a gente mencionou mas por hora nesse cenário usuário extrai os dados manualmente joga no store e aí a gente vai ter três casos neiry Time streaming e Beth e aí eu vou passar os três desenhos três explicações e em seguida a gente faz um desses exemplos que vai se unir ao time implantando ele nos GCP etc e tal 

Bom ambos os cenários vão ser feitos no GCP todos os cenários tem como objetivo ali pegar os dados nessa fonte transformar o Beth e o New time são cenários mais apropriados pra esse cenário mas eu vou passar uma arquitetura de referência streaming só pra não ficar faltando então vamos supor que esses dados Eles vêm dessa planilha mas nesse caso vamos supor que eles venham de um tópico ou de alguma pi alguma coisa que precisa ser consultada em 1000 segundos ou em segundos né então em segundos a gente provavelmente acabaria precisando de uma arquitetura streaming não é esse o cenário não é esse o cenário claro mas a gente precisa mas eu vou passar essa arquitetura de referência só pra entender caso extração fosse diferente tá
Agora pra gente estruturar primeiro a arquitetura streaming tá que é um caso que não é o caso recomendado pra esse tipo de distração contudo vou deixar arquitetura de referência está pro streaming usando GCP a gente nós vamos estar supondo aí que a gente vai estar recebendo as informações via mensajería tá a gente está falando de streaming então provavelmente ia consumir um tópico ou consumir alguma PI ali em tempo real e essas essas requisições ou essas mensagens que chegariam elas seriam inseridas dentro do PIS então eu acho que tiraria as mensagens de um tópico que poderia ficar em qualquer Claudio seja a hora com a WSGCP enfim qualquer iCloud e é o extrairia essas mensagens caso Castiano era pra dentro do PSB ou as requisições de app que eu fizesse a app em tempo real eu comia também pra dentro do Bob SP porque ele é o serviço de mensageiro principal do GCP ali e dentro dele a gente pode criar recursos como prediletos e outros recursos pra ficar investigando e em visualizando enfim todas as métricas todos os as mensagens e o que mais puder acontecerentão pro primeiro cenário a gente faria um streaming aonde após Casca as mensagens para o PIS a gente teria um Data Flow esse Datafolha esse Datafolha extrairia mensagem esse Data flu extrairia mensagem esse Data pegar trairia a mensagem esse Data Leiria a mensagem do tópico do BSB que é um casca por trás né leia mensagem do tópico do BSB transformaria o adaptar o que fosse necessário on-line então se a gente tá falando de tempo real isso me usaria um link ali junto com a Pax Bim pra poder transformar em tempo real dado enquanto ele viaja tá de transformando esse dado em tempo real se necessário eu inseriria esses dados dentro do uma tabela do BC streaming então a gente pediu três tabelas ali diferentes não foi falado de modelagem real trânsito de Ref de estar esquema Snow falei que enfim não foi falado de modelagem então a gente imagina que o nosso processo ele vai ter carregar os dados na Woll ou na bronze digamos assim né na camada zero na camada bruta tá então a gente receberia esses dados extrairia esses dados de uma fonte e colocaria numa camada bruta então o Data poderia fazer possíveis adaptações e transformações dentro dos dados se necessário inseriria dentro da tabela do Big car streaming esse fluxo custa caro mas ele funciona uma latência muito baixa em tempo real então nesse primeiro cenário a gente teria estar utilizando essas ferramentas pra poder estar coordenando esse fluxo de mensagens em

Pro segundo cenário a gente trabalharia com com Ne Real Time ou seja próximo do tempo real o que que significa isso significa que ele está entre o Betti Stremme ele tem uma latência de segundos digamos assim às vezes minutos enfim esse cenário do New Real Time a gente trabalharia utilizando uma outro tipo de modelagem no exemplo anterior a gente traz os dados de um tópico do pub sobe e coloca dentro do Big então aquela primeira aquele primeiro contato do Big Carlos seria a nossa camada land ou nossa camada Round digamos assim né ou enfim nossa camada bronze seria nossa camada que é uma réplica é uma foto daquilo que a gente recebe na mensage Ariah nesse segundo caso do New Time como a gente já tem o dado no histórias já que ela já é a nossa camada land então a nossa ideia é puxar o dado de lá né extrair esse dado de lá de uma forma né que ele vai ser orquestrado né enfim ele vai ser extraído e utilizado cansei de falar esse dado vai ser extraído do Storage vai ser líquido da nossa camada land digamos assim né e seria inserido numa camada que seria uma camada Silver ou uma camada Ro depende do da da abordagem mas a ideia seria a nossa camada do Stories já é um espelho da origem então a gente já poderia fazer algumas transformações etc e tal como nesse caso não foi pedido a gente vai trazer o da do mesmo jeito do jeito que vem da origem a gente vai fazer algumas transformações e vai colocar nessa primeira camada que seria uma camada crua ou uma camada Silver enfim aí eu vou consultar no documentação pra passar mas essa criatura de New time a gente teria outras ferramentas tá eu gostaria de ter um orquestra Dorr tá ou um gatilho então tem duas maneiras de fazer esse cenário de New Time ou usando um orquestra Dorr como


O terceiro cena tá me chamando o terceiro cenário a gente vai utilizar como arquitetura Beth com orquestra door air Flow e o Air Flow ele vai ser acionado aí no caso porque a gente pode utilizar a gente pode usar os Trigger Ears do GCP com o Cloud por exemplo pra quando o arquivo cai no back de gente processar ele direto né ou a gente pode usar o Cláudio pra invocar uma adega aí depende da arquitetura depende do que cê vai usar pra não ficar redundante mas a gente pode usar tanto caos que haja contra o ELLLO pra fazer esse c

Ou seja podemos usar o Air Flow pra o que está aí a gente poderia seguir lá Cherry a cada período então por exemplo eu sei que eu vou fazer a do download dos arquivos às seis poderia colocar daí pra rodar 6h10 6h15 6h30 enfim provavelmente uma vez por dia né Como os dados estão por dia a gente entende que vai ter uma estação estação por dia e uma adega rodar uma vez por dia com próprio quer ajuda dela ou a gente poderia fazer usando um Trigger do GC pra invocar Dag né ou o próprio Clouds que ajuda néque quando rodasse ele rodaria pipeline e pipeline criaria data pro e afins tá então como é que a gente utilizaria só o UFF ou a gente utilizaria o Clouds Catch com Y tá pra fazer esse engatilha
